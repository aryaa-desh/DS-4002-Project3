{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e82d5-c7bd-48fa-89e4-f9e8048c3f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script trains and runs the model along with computing the evalutation criteria. \n",
    "# This script can take several hours to run if you do not have access to strong computing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43eb48f-8e7a-4056-b165-525e3d3351c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import tensorflow as tf\n",
    "import keras \n",
    "import numpy as np\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4d0871c-7ac3-4a6a-b816-1de90149f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a base model\n",
    "base_model = EfficientNetB7(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "# freeze the base model layer \n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce6cd4c-5b48-430a-a71e-f32765d06c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'Data' already exists.\n"
     ]
    }
   ],
   "source": [
    "# splitting the data into train, validation, and test\n",
    "img_size = (224, 224)\n",
    "shuffle_value = True\n",
    "batch_size = 32\n",
    "seed = 123\n",
    "validation_split = 0.3\n",
    "\n",
    "# creating a directory of the data \n",
    "data_dir = \"Data\"\n",
    "try:\n",
    "    os.makedirs(data_dir)\n",
    "    print(f\"Directory '{data_dir}' created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Directory '{data_dir}' already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "936fbb87-4cc4-4985-9957-b8b9493358c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9952 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# loading all of the data\n",
    "full_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# define class names/number of classes for model later\n",
    "class_names = full_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "\n",
    "# get test/validation/train sizes\n",
    "total_size = tf.data.experimental.cardinality(full_ds).numpy()\n",
    "test_size = int(0.15 * total_size)\n",
    "val_size = int((15/85) * (total_size - test_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8764100d-ab5e-4c35-a569-d390d2671196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data\n",
    "\n",
    "test_ds = full_ds.take(test_size)\n",
    "train_val_ds = full_ds.skip(test_size)\n",
    "\n",
    "val_ds = train_val_ds.take(val_size)\n",
    "train_ds = train_val_ds.skip(val_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b309f80-eba6-45fd-8b41-1bdf01361778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model \n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61ebbe4c-816c-497e-80c2-89f6a1240db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling model \n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e2c7a35-b858-4209-84de-a16670d5f8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "219/219 [==============================] - 1445s 6s/step - loss: 0.5737 - accuracy: 0.6929 - val_loss: 0.4908 - val_accuracy: 0.7683\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 1401s 6s/step - loss: 0.5100 - accuracy: 0.7399 - val_loss: 0.4695 - val_accuracy: 0.7779\n",
      "Epoch 3/10\n",
      "219/219 [==============================] - 1405s 6s/step - loss: 0.4894 - accuracy: 0.7464 - val_loss: 0.4516 - val_accuracy: 0.7996\n",
      "Epoch 4/10\n",
      "219/219 [==============================] - 1388s 6s/step - loss: 0.4639 - accuracy: 0.7671 - val_loss: 0.4444 - val_accuracy: 0.7731\n",
      "Epoch 5/10\n",
      "219/219 [==============================] - 1392s 6s/step - loss: 0.4617 - accuracy: 0.7684 - val_loss: 0.4289 - val_accuracy: 0.7942\n",
      "Epoch 6/10\n",
      "219/219 [==============================] - 1402s 6s/step - loss: 0.4378 - accuracy: 0.7881 - val_loss: 0.4293 - val_accuracy: 0.7976\n",
      "Epoch 7/10\n",
      "219/219 [==============================] - 1390s 6s/step - loss: 0.4288 - accuracy: 0.7952 - val_loss: 0.4335 - val_accuracy: 0.7887\n",
      "Epoch 8/10\n",
      "219/219 [==============================] - 1382s 6s/step - loss: 0.4154 - accuracy: 0.7994 - val_loss: 0.4186 - val_accuracy: 0.8050\n",
      "Epoch 9/10\n",
      "219/219 [==============================] - 1361s 6s/step - loss: 0.4103 - accuracy: 0.8018 - val_loss: 0.4111 - val_accuracy: 0.8064\n",
      "Epoch 10/10\n",
      "219/219 [==============================] - 1361s 6s/step - loss: 0.4032 - accuracy: 0.8129 - val_loss: 0.4254 - val_accuracy: 0.7928\n"
     ]
    }
   ],
   "source": [
    "# model fitting \n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0277ffd-2de0-43c0-84c6-33f2cecc3e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreezing the base model layer for fine tuning\n",
    "'''\n",
    "base_model.trainable = True\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Lower LR for fine-tuning\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70f151f6-2393-47a2-95d4-ed2ac010224a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 239s 5s/step - loss: 0.4356 - accuracy: 0.7969\n",
      "Test Accuracy: 0.7969\n"
     ]
    }
   ],
   "source": [
    "# evaluating accuracy  on the test set \n",
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91dd2502-2b04-4e02-a921-74556c74aa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 11s 11s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Toxic1       0.86      0.71      0.78       739\n",
      "      Toxic        0.75      0.89      0.82       733\n",
      "\n",
      "    accuracy                           0.80      1472\n",
      "   macro avg       0.81      0.80      0.80      1472\n",
      "weighted avg       0.81      0.80      0.80      1472\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluating precision on the test set \n",
    "\n",
    "# define true and predicted y values \n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# run the model on the test images \n",
    "for images, labels in test_ds:\n",
    "    preds = model.predict(images)\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "# get the classification report \n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f03097d6-7c87-4b4d-9144-bb586d323213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a confusion matrix \n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# ploting confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# save confusion matrix as a png\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481bcfe-e21b-4b89-bcb2-ad1d8d67cf70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.17.0",
   "language": "python",
   "name": "tensorflow-2.17.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
